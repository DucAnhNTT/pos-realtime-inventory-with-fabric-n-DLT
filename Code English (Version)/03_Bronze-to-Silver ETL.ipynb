{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ec90979-0a95-4948-8a48-d3a4d9906b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You may find this series of notebooks at https://github.com/databricks-industry-solutions/pos-dlt. For more information about this solution accelerator, visit https://www.databricks.com/solutions/accelerators/real-time-point-of-sale-analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b34da4d-23c0-4393-a8d4-52d3127ffc33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook was developed to run as part of a Delta Live Table (DLT) pipeline. Details on the scheduling of the DLT jobs are provided in the *POS 05* notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "259d40d6-b8a2-4ab0-8b2a-49e651f9f99d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**IMPORTANT NOTE** Do not attempt to interactively run the code in this notebook.  **This notebook must be scheduled as a DLT pipeline.**  If you attempt to run the notebook interactively, *e.g.* by running individual cells or clicking *Run All* from the top of the notebook you will encounter errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8da9901f-6987-45e4-8bc9-ec8022b2a4c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The purpose of this notebook is to process inventory change event and snapshot data being transmitted into the Azure infrastructure from the various (simulated) point-of-sale systems in this demonstration.  As they are received, these data are landed into various Delta tables, enabling persistence and downstream stream processing.\n",
    "\n",
    "This notebook should be scheduled to run while the *POS 02* notebook (which generates the simulated event data) runs on a separate cluster. It also depends on the demo environment having been configured per the instructions in the *POS 01* notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72a6f453-418e-490c-a73c-93925fd50b53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In order to enable the definition of Delta Live Table (DLT) objects, we need to import the *dlt* library in addition to other libraries we might reference in our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ec0c620-05f9-46df-be4b-f57c8ccd0ea2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Required Libraries"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from delta.tables import *\n",
    "\n",
    "import dlt # this is the delta live tables library\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "634973a1-40f8-4681-ad9f-b810f752eb0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notebooks scheduled for the purpose of running DLT workflows must be self-contained, *i.e.* they cannot reference other notebooks.  Given this requirement, we'll set a few configuration settings here.  You should copy these values from the *POS 01* notebook.\n",
    "\n",
    "As mentioned in the *POS 01* notebook, many of these values are sensitive and should be managed using [Databricks secrets](https://docs.databricks.com/security/secrets/index.html). Secrets are not being addressed here for the purpose of transparency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b1fb55e-4203-4469-8011-af6ec94628d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialize Configuration"
    }
   },
   "outputs": [],
   "source": [
    "config = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2190990e-419c-4163-a30a-1022e66ed125",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Config Values Copied from POS 01"
    }
   },
   "outputs": [],
   "source": [
    "# mount point associated with our data files\n",
    "config['dbfs_mount_name'] = f'/mnt/pos'\n",
    "\n",
    "# iot hub config\n",
    "config['iot_device_connection_string'] = dbutils.secrets.get(\"solution-accelerator-cicd\",\"rcg_pos_iot_hub_conn_string\")# 'OR YOUR IOT HUB DEVICE CONNECTION STRING HERE'\n",
    "config['event_hub_compatible_endpoint'] = dbutils.secrets.get(\"solution-accelerator-cicd\",\"rcg_pos_iot_hub_endpoint\") # 'YOUR IOT HUB EVENT HUB COMPATIBLE ENDPOINT PROPERTY HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f604f51-b2b5-41ae-a4d4-0bd0e5b34e20",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Config Values Derived from Other Config"
    }
   },
   "outputs": [],
   "source": [
    "# location of our static data files\n",
    "config['stores_filename'] = config['dbfs_mount_name'] + '/static_data/store.txt'\n",
    "config['items_filename'] = config['dbfs_mount_name'] + '/static_data/item.txt'\n",
    "config['change_types_filename'] = config['dbfs_mount_name'] + '/static_data/inventory_change_type.txt'\n",
    "\n",
    "#location of our inventory snapshot files\n",
    "config['inventory_snapshot_path'] = config['dbfs_mount_name'] + '/inventory_snapshots/'\n",
    "\n",
    "# helper function to convert strings above into dictionaries\n",
    "def split_connstring(connstring):\n",
    "  conn_dict = {}\n",
    "  for kv in connstring.split(';'):\n",
    "    k,v = kv.split('=',1)\n",
    "    conn_dict[k]=v\n",
    "  return conn_dict\n",
    "  \n",
    "# split conn strings\n",
    "iothub_conn = split_connstring(config['iot_device_connection_string'])\n",
    "eventhub_conn = split_connstring(config['event_hub_compatible_endpoint'])\n",
    "\n",
    "# configure kafka endpoint settings\n",
    "config['eh_namespace'] = eventhub_conn['Endpoint'].split('.')[0].split('://')[1] \n",
    "config['eh_kafka_topic'] = iothub_conn['HostName'].split('.')[0]\n",
    "config['eh_listen_key_name'] = 'ehListen{0}AccessKey'.format(config['eh_namespace'])\n",
    "config['eh_bootstrap_servers'] = '{0}.servicebus.windows.net:9093'.format(config['eh_namespace'])\n",
    "config['eh_sasl'] = 'kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"$ConnectionString\\\" password=\\\"Endpoint={0};SharedAccessKeyName={1};SharedAccessKey={2}\\\";'.format(eventhub_conn['Endpoint'], eventhub_conn['SharedAccessKeyName'], eventhub_conn['SharedAccessKey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c13a8bb-8b62-4dee-b466-ed425621f958",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Setup the POS Database Environment\n",
    "\n",
    "The typical first step in setting up a streaming architecture is to create a database to house our tables.  This needs to be done in advance of running the DLT jobs:\n",
    "```\n",
    "CREATE DATABASE IF NOT EXISTS pos_dlt;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "359c9758-17bd-46e0-8888-d11802aad334",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In defining the DLT tables later in this notebook, you may notice we are not referencing a specific database name.  Instead, when we schedule our DLT jobs (in *POS 05*), we'll specify a target database for these objects, assigning them to the database as that time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56d40721-9fb3-420e-9af1-15c259cceb51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2: Load the Static Reference Data\n",
    "\n",
    "While we've given attention in this and other notebooks to the fact that we are receiving streaming event data and periodic snapshots, we also have reference data we need to access.  These data are relatively stable so that we might update them just once daily. \n",
    "\n",
    "To define a Delta Live Tables object, we define a dataframe as we would normally in Spark and have that dataframe returned by a function. A *@dlt.table* decorator on the function identifies it as defining a DLT object and provides additional metadata such as a description (*comment*) and any other metadata we might find useful for the management of our DLT workflows.  \n",
    "\n",
    "**NOTE** For more information on the DLT Python specification, please refer to [this document](https://docs.microsoft.com/en-us/azure/databricks/data-engineering/delta-live-tables/delta-live-tables-python-ref).\n",
    "\n",
    "The *name* element associated with the *@dlt.table* decorator identifies the name of the table to be created by the DLT engine.  Had we not used this optional element, the function name would have been used as the name of the resulting table. \n",
    "\n",
    "The *spark_conf* element associated with the DLT table defines the frequency with which the table will be run.  At the passing of each interval, the DTL engine will examine the file pointed to by this dataframe to see if there have been any changes.  If there have been, the table will be rewritten using the data processing logic associated with the dataframe definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06d4c5e2-899f-42a6-8634-704ba8ab5e24",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stores"
    }
   },
   "outputs": [],
   "source": [
    "# define schema for incoming file\n",
    "store_schema = StructType([\n",
    "  StructField('store_id', IntegerType()),\n",
    "  StructField('name', StringType())\n",
    "  ])\n",
    "\n",
    "#define the dlt table\n",
    "@dlt.table(\n",
    "  name='store', # name of the table to create\n",
    "  comment = 'data associated with individual store locations', # description\n",
    "  table_properties = {'quality': 'silver'}, # various table properties\n",
    "  spark_conf = {'pipelines.trigger.interval':'24 hours'} # various spark configurations\n",
    "  )\n",
    "def store():\n",
    "  df = (\n",
    "      spark\n",
    "      .read\n",
    "      .csv(\n",
    "        config['stores_filename'], \n",
    "        header=True, \n",
    "        schema=store_schema\n",
    "        )\n",
    "      )\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "857c6ae6-3d14-4f9a-b15c-53f96178614a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The DLT table definition is not run in an interactive mode.  Instead, the notebook that contains it is scheduled and runs as part of a background job.  To test dataframe definitions, a common practice is to write the basic dataframe definitions and display their results in another notebook run as part of an interactive session:\n",
    "\n",
    "```\n",
    "store_schema = StructType([\n",
    "  StructField('store_id', IntegerType()),\n",
    "  StructField('name', StringType())\n",
    "  ])\n",
    "\n",
    "def store():\n",
    "  df = (\n",
    "      spark\n",
    "      .read\n",
    "      .csv(\n",
    "        config['stores_filename'], \n",
    "        header=True, \n",
    "        schema=store_schema\n",
    "        )\n",
    "      )\n",
    "  return df\n",
    "\n",
    "display( store() )\n",
    "```\n",
    "\n",
    "\n",
    "Once the function is validated, it can then be pasted into the notebook used to define the DLT workflows and decorators attached. \n",
    "\n",
    "With the first of our DLT tables defined, we can now write a similar DTL table definition for our item data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "538c6fef-2a77-4671-a5cf-a42f92776f25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Items"
    }
   },
   "outputs": [],
   "source": [
    "item_schema = StructType([\n",
    "  StructField('item_id', IntegerType()),\n",
    "  StructField('name', StringType()),\n",
    "  StructField('supplier_id', IntegerType()),\n",
    "  StructField('safety_stock_quantity', IntegerType())\n",
    "  ])\n",
    "\n",
    "@dlt.table(\n",
    "  name = 'item',\n",
    "  comment = 'data associated with individual items',\n",
    "  table_properties={'quality':'silver'},\n",
    "  spark_conf={'pipelines.trigger.interval':'24 hours'}\n",
    ")\n",
    "def item():\n",
    "  return (\n",
    "    spark\n",
    "      .read\n",
    "      .csv(\n",
    "        config['items_filename'], \n",
    "        header=True, \n",
    "        schema=item_schema\n",
    "        )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "373916c0-617f-49b9-9731-85a243a37a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "And lastly, we can write a DLT table definition for our inventory change type data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5816ef7b-f998-44f2-a996-c44297afee23",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inventory Change Types"
    }
   },
   "outputs": [],
   "source": [
    "change_type_schema = StructType([\n",
    "  StructField('change_type_id', IntegerType()),\n",
    "  StructField('change_type', StringType())\n",
    "  ])\n",
    "\n",
    "@dlt.table(\n",
    "  name = 'inventory_change_type',\n",
    "  comment = 'data mapping change type id values to descriptive strings',\n",
    "  table_properties={'quality':'silver'},\n",
    "  spark_conf={'pipelines.trigger.interval':'24 hours'}\n",
    ")\n",
    "def inventory_change_type():\n",
    "  return (\n",
    "    spark\n",
    "      .read\n",
    "      .csv(\n",
    "        config['change_types_filename'],\n",
    "        header=True,\n",
    "        schema=change_type_schema\n",
    "        )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c08a625a-5310-4772-b3b1-0a929dfc8888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Stream Inventory Change Events\n",
    "\n",
    "Let's now tackle our inventory change event data. These data consist of a JSON document transmitted by a store to summarize an event with inventory relevance. These events may represent sales, restocks, or reported loss, damage or theft (shrinkage).  A fourth event type, *bopis*, indicates a sales transaction that takes place in the online store but which is fulfilled by a physical store. All these events are transmitted as part of a consolidated stream:</p>\n",
    "\n",
    "<img src='https://brysmiwasb.blob.core.windows.net/demos/images/pos_event_change_streaming_etl_UPDATED.png' width=600>\n",
    "\n",
    "Just as before, we write a function to return a Spark dataframe and decorate that function with the appropriate elements.  The dataframe is defined using patterns used with Spark Structured Streaming.  Because the dataframe is streaming data from the Kafka endpoint of the Azure IOT Hub, we configure the connection using Kafka properties.  As a Kafka data source, the structure of the dataframe read from the IOT Hub is pre-defined (so that there's no need to specify a schema). The *maxOffsetsPerTrigger* configuration setting limits the number of messages read from the IOT Hub within a given cycle so that we don't overwhelm the resources provisioned for stream processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "371f00db-9d75-4648-8fd5-543783467799",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read Event Stream"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "  name = 'raw_inventory_change',\n",
    "  comment= 'data representing raw (untransformed) inventory-relevant events originating from the POS',\n",
    "  table_properties={'quality':'bronze'}\n",
    "  )\n",
    "def raw_inventory_change():\n",
    "  return (\n",
    "    spark\n",
    "      .readStream\n",
    "      .format('kafka')\n",
    "      .option('subscribe', config['eh_kafka_topic'])\n",
    "      .option('kafka.bootstrap.servers', config['eh_bootstrap_servers'])\n",
    "      .option('kafka.sasl.mechanism', 'PLAIN')\n",
    "      .option('kafka.security.protocol', 'SASL_SSL')\n",
    "      .option('kafka.sasl.jaas.config', config['eh_sasl'])\n",
    "      .option('kafka.request.timeout.ms', '60000')\n",
    "      .option('kafka.session.timeout.ms', '60000')\n",
    "      .option('failOnDataLoss', 'false')\n",
    "      .option('startingOffsets', 'latest')\n",
    "      .option('maxOffsetsPerTrigger', '100') # read 100 messages at a time\n",
    "      .load()\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91f2a3ae-1569-4d7b-8825-7bc918c60d6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The schema of the data being read through the Kafka connector is pre-defined as follows:\n",
    "\n",
    "| Column | Type |\n",
    "|--------|------|\n",
    "| key| binary |\n",
    "| value | binary |\n",
    "| topic | string |\n",
    "| partition | int |\n",
    "| offset | long|\n",
    "| timestamp | timestamp|\n",
    "| timestampType | int|\n",
    "| headers | array|\n",
    "\n",
    "The *value* field represents the payload sent from the simulated POS.  To access this data, we need to cast and transform the data leveraging advance knowledge of its structure.  In our scenario, this data is delivered as JSON with a well-defined schema.  We can convert this data and extract elements from the *value* field using standard dataframe method calls as one would employ with Structured Streaming.  Please note that because the table builds on the *raw_inventory_change* DLT object defined in the last cell, we use the *dlt.read_stream()* method to access its data. This instructs the DLT engine to treat the DLT object, *i.e.* *inventory_change*, as part of the same streaming pipeline as the referenced object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7e8c46c-86a3-4f51-8c05-76dde279a324",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Convert Transaction to Structure Field & Extract Data Elements"
    }
   },
   "outputs": [],
   "source": [
    "# schema of value field\n",
    "value_schema = StructType([\n",
    "  StructField('trans_id', StringType()),\n",
    "  StructField('store_id', IntegerType()),\n",
    "  StructField('date_time', TimestampType()),\n",
    "  StructField('change_type_id', IntegerType()),\n",
    "  StructField('items', ArrayType(\n",
    "    StructType([\n",
    "      StructField('item_id', IntegerType()), \n",
    "      StructField('quantity', IntegerType())\n",
    "      ])\n",
    "    ))\n",
    "  ])\n",
    "\n",
    "# define inventory change data\n",
    "@dlt.table(\n",
    "  name = 'inventory_change',\n",
    "  comment = 'data representing item-level inventory changes originating from the POS',\n",
    "  table_properties = {'quality':'silver'}\n",
    ")\n",
    "def inventory_change():\n",
    "  df = (\n",
    "    dlt\n",
    "      .read_stream('raw_inventory_change')\n",
    "      .withColumn('body', f.expr('cast(value as string)')) # convert payload to string\n",
    "      .withColumn('event', f.from_json('body', value_schema)) # parse json string in payload\n",
    "      .select( # extract data from payload json\n",
    "        f.col('event').alias('event'),\n",
    "        f.col('event.trans_id').alias('trans_id'),\n",
    "        f.col('event.store_id').alias('store_id'), \n",
    "        f.col('event.date_time').alias('date_time'), \n",
    "        f.col('event.change_type_id').alias('change_type_id'), \n",
    "        f.explode_outer('event.items').alias('item')     # explode items so that there is now one item per record\n",
    "        )\n",
    "      .withColumn('item_id', f.col('item.item_id'))\n",
    "      .withColumn('quantity', f.col('item.quantity'))\n",
    "      .drop('item')\n",
    "      .withWatermark('date_time', '1 hour') # ignore any data more than 1 hour old flowing into deduplication\n",
    "      .dropDuplicates(['trans_id','item_id'])  # drop duplicates \n",
    "    )\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9140eae7-cb4d-4e3d-b121-b6f4a40b2333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4: Stream Inventory Snapshots\n",
    "\n",
    "Periodically, we receive counts of items in inventory at a given store location.  Such inventory snapshots are frequently used by retailers to update their understanding of which products are actually on-hand given concerns about the reliability of calculated inventory quantities.  We may wish to preserve both a full history of inventory snapshots received and the latest counts for each product in each store location.  To meet this need, two separate tables are built from this one data source as it arrives in our environment.\n",
    "\n",
    "<img src='https://brysmiwasb.blob.core.windows.net/demos/images/pos_snapshot_auto_loader_etl_UPDATED.png' width=600>\n",
    "\n",
    "These inventory snapshot data arrive in this environment as CSV files on a slightly irregular basis. But as soon as they land, we will want to process them, making them available to support more accurate estimates of current inventory. To enable this, we will take advantage of the Databricks [Auto Loader](https://docs.databricks.com/spark/latest/structured-streaming/auto-loader.html) feature which listens for incoming files to a storage path and processes the data for any newly arriving files as a stream.  Again, we define a function to return a Spark Structured Streaming dataframe and decorate that function to register it with the DLT engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ccf2e94-13c6-4c73-9295-958a647797d7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Access Incoming Snapshots"
    }
   },
   "outputs": [],
   "source": [
    "inventory_snapshot_schema = StructType([\n",
    "  StructField('id', IntegerType()),\n",
    "  StructField('item_id', IntegerType()),\n",
    "  StructField('employee_id', IntegerType()),\n",
    "  StructField('store_id', IntegerType()),\n",
    "  StructField('date_time', TimestampType()),\n",
    "  StructField('quantity', IntegerType())\n",
    "  ])\n",
    "\n",
    "@dlt.table(\n",
    "  name='inventory_snapshot',\n",
    "  comment='data representing periodic counts of items in inventory',\n",
    "  table_properties={'quality':'silver'}\n",
    "  )\n",
    "def inventory_snapshot():\n",
    "  return (\n",
    "    spark\n",
    "      .readStream\n",
    "      .format('cloudFiles')  # auto loader\n",
    "      .option('cloudFiles.format', 'csv')\n",
    "      .option('cloudFiles.includeExistingFiles', 'true') \n",
    "      .option('header', 'true')\n",
    "      .schema(inventory_snapshot_schema)\n",
    "      .load(config['inventory_snapshot_path'])\n",
    "      .drop('id')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8f85691-d7f8-4230-a4e4-193d2b77a780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The *inventory_snapshot* table will contain details about every inventory count taken within a given store location.  For the purposes of calculating things like current inventory, we only need the latest count of an item in a given location.  We can create a DLT table containing this subset of data in a relatively simple manner using the [*apply_changes()*](https://docs.microsoft.com/en-us/azure/databricks/data-engineering/delta-live-tables/delta-live-tables-cdc) method.\n",
    "\n",
    "The apply_changes() method is part of Delta Live Table's change data capture functionality.  While we could accomplish this update with a merge, this is such a common pattern the DLT builds in mechanics to handle it in a more succinct manner.  With a source data *stream* and *target* table identified, *keys* for matching records are specified.  When there's a match, the row is updated based on the latest *sequence_by* value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2f0f63d-70c1-4a21-b4fc-a40ad5ce4ff0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Table for Latest Snapshot Data"
    }
   },
   "outputs": [],
   "source": [
    "# create dlt table to hold latest inventory snapshot (if it doesn't exist)\n",
    "dlt.create_target_table('latest_inventory_snapshot')\n",
    "\n",
    "# merge incoming snapshot data with latest\n",
    "dlt.apply_changes( # merge\n",
    "  target = 'latest_inventory_snapshot',\n",
    "  source = 'inventory_snapshot',\n",
    "  keys = ['store_id','item_id'], # match source to target records on these keys\n",
    "  sequence_by = 'date_time' # determine latest value by comparing date_time field\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "109fb78d-9c45-414f-905d-24ee878e410a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If you are familiar with the earlier release of this accelerator which is based on structured streaming, you may remember that to accomplish the work in the last cell, we implemented *forEachBatch()* logic.  In that step, we not only defined a merge operation but also inserted *dummy records* into the *inventory_change* table to work around a problem with streaming joins that occurs as we move to construct the current inventory dataset.  With our DLT implementation of current inventory (in *POS 04*), we don't have this same limitation.  This allows us to simplify our logic here in the Bronze-to-Silver ETL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6cbacce-3beb-4dfb-9252-cca857e04857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2022 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the [Databricks License](https://databricks.com/db-license-source).  All included or referenced third party libraries are subject to the licenses set forth below.\n",
    "\n",
    "| library                                | description             | license    | source                                              |\n",
    "|----------------------------------------|-------------------------|------------|-----------------------------------------------------|\n",
    "| azure-iot-device                                     | Microsoft Azure IoT Device Library | MIT    | https://pypi.org/project/azure-iot-device/                       |\n",
    "| azure-storage-blob                                | Microsoft Azure Blob Storage Client Library for Python| MIT        | https://pypi.org/project/azure-storage-blob/      |"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_Bronze-to-Silver ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
