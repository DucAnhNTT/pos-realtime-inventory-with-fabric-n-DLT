{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f21e817e-b3e2-4ee3-8f57-63b919929835",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You may find this series of notebooks at https://github.com/databricks-industry-solutions/pos-dlt. For more information about this solution accelerator, visit https://www.databricks.com/solutions/accelerators/real-time-point-of-sale-analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a200912-23bc-4cd5-896c-be1da81c9c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook was developed to run on a minimal (one-worker node) cluster. You should leave this notebook running on this cluster while you run the remaining notebooks as a Delta Live Tables job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94281181-76f2-4aed-9ea3-30dc6cdee313",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The purpose of this notebook is to generate a stream of inventory-relevant data originating from two simulated stores, one physical and the other online.  These data are transmitted to various data ingest services configured with the cloud provider as indicated in the *POS 01* notebook.\n",
    "\n",
    "The first of the two streams of data being generated represents inventory change events captured by a store's point-of-sale system. The second of these streams represents the point-in-time quantities (snapshot) of products in a store as determined through a manual count of inventory. Due to differences in the frequency with which each type of data is generated and the volume of data associated with each, they are transmitted in different formats using different delivery mechanisms.\n",
    "</p>\n",
    "<img src='https://brysmiwasb.blob.core.windows.net/demos/images/pos_data_generation.png' width=400>\n",
    "\n",
    "**IMPORTANT NOTE** The Azure IOT Hub is not reset between runs of this notebook.  As a result, it is possible that the messages delivered from prior runs could still be read by downstream streaming jobs.  To ensure a clean environment between runs, you may wish to first delete your Azure IOT Hub deployment and then recreate it.  This will require you to repeat the steps outlined in *POS 01* and update appropriate configuration values in that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "076c080a-e57b-4e89-80bf-7d914c7af276",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Required Libraries"
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-iot-device==2.7.1 --use-feature=2020-resolver\n",
    "%pip install azure-storage-blob==12.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38599e71-9f38-4297-ba5a-d57a6b7144ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"mode\", \"prod\")\n",
    "mode = dbutils.widgets.get(\"mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6e54ebe-e967-4fc3-81e5-9d27c9afeacb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import Required Libraries"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "import datetime, time\n",
    "\n",
    "from azure.iot.device import IoTHubDeviceClient\n",
    "from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "454c893c-378d-42e8-b823-08fc113f4362",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Notebook Configuration"
    }
   },
   "outputs": [],
   "source": [
    "%run \"./01_Environment Setup\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b25c54c-897b-4728-95b1-3783037b3cf3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This notebook is typically run to simulate a new stream of POS data. To ensure data from prior runs are not used in downstream calculations, you should reset the database and DLT engine environment between runs of this notebook:\n",
    "\n",
    "**NOTE** These actions are not typical of a real-world workflow but instead are here to ensure the proper calculation of values in a simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "490d426e-3094-4232-94cf-5c2d8fd03e26",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reset the Target Database"
    }
   },
   "outputs": [],
   "source": [
    "_ = spark.sql(\"DROP DATABASE IF EXISTS {0} CASCADE\".format(config['database']))\n",
    "_ = spark.sql(\"CREATE DATABASE IF NOT EXISTS {0}\".format(config['database']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beeb27fa-e560-471d-b5e9-1a766cfe2b68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**NOTE** This next step should be run before any DLT jobs are launched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb9d38e5-9612-458f-b2b5-a566c35fd6f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reset the DLT Environment"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(config['dlt_pipeline'],True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6d343f8-4141-4afb-829e-ca8e6d0657cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1: Assemble Inventory Change Records\n",
    "\n",
    "The inventory change records represent events taking place in a store location which impact inventory.  These may be sales transactions, recorded loss, damage or theft, or replenishment events. In addition, Buy-Online, Pickup In-Store (BOPIS) events originating in the online store and fulfilled by the physical store are captured in the data. While each of these events might have some different attributes associated with them, they have been consolidated into a single stream of inventory change event records. Each event type is distinguished through a change type identifier.\n",
    "\n",
    "A given event may involve multiple products (items).  By grouping the data for items associated with an event around the transaction ID that uniquely identifies that event, the multiple items associated with a sales transaction or other event type can be efficiently transmitted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f72cc450-cae3-4e62-84ff-8d08714f2003",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Combine & Reformat Inventory Change Records"
    }
   },
   "outputs": [],
   "source": [
    "# format of inventory change records\n",
    "inventory_change_schema = StructType([\n",
    "  StructField('trans_id', StringType()),  # transaction event ID\n",
    "  StructField('item_id', IntegerType()),  \n",
    "  StructField('store_id', IntegerType()),\n",
    "  StructField('date_time', TimestampType()),\n",
    "  StructField('quantity', IntegerType()),\n",
    "  StructField('change_type_id', IntegerType())\n",
    "  ])\n",
    "\n",
    "# inventory change record data files (one from each store)\n",
    "inventory_change_files = [\n",
    "  config['inventory_change_store001_filename'],\n",
    "  config['inventory_change_online_filename']\n",
    "  ]\n",
    "\n",
    "# read inventory change records and group items associated with each transaction so that one output record represents one complete transaction\n",
    "inventory_change = (\n",
    "  spark\n",
    "    .read\n",
    "    .csv(\n",
    "      inventory_change_files, \n",
    "      header=True, \n",
    "      schema=inventory_change_schema, \n",
    "      timestampFormat='yyyy-MM-dd HH:mm:ss'\n",
    "      )\n",
    "    .withColumn('trans_id', f.expr('substring(trans_id, 2, length(trans_id)-2)')) # remove surrounding curly braces from trans id\n",
    "    .withColumn('item', f.struct('item_id', 'quantity')) # combine items and quantities into structures from which we can build a list\n",
    "    .groupBy('date_time','trans_id')\n",
    "      .agg(\n",
    "        f.first('store_id').alias('store_id'),\n",
    "        f.first('change_type_id').alias('change_type_id'),\n",
    "        f.collect_list('item').alias('items')  # organize item info as a list\n",
    "        )\n",
    "    .orderBy('date_time','trans_id')\n",
    "    .toJSON()\n",
    "    .collect()\n",
    "  )\n",
    "\n",
    "# print a single transaction record to illustrate data structure\n",
    "eval(inventory_change[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0b01cd3-8740-46a0-954e-210eaa7a5f44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Step 2: Assemble Inventory Snapshots\n",
    "\n",
    "Inventory snapshots represent the physical counts taken of products sitting in inventory.  \n",
    "Such counts are periodically taken to capture the true state of a store's inventory and are necessary given the challenges most retailers encounter in inventory management.\n",
    "\n",
    "With each snapshot, we capture basic information about the store, item, and quantity on-hand along with the date time and employee associated with the count.  So that the impact of inventory snapshots may be more rapidly reflected in our streams, we simulate a complete recount of products in a store every 5-days.  This is far more aggressive than would occur in the real world but again helps to demonstrate the streaming logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f06ab2a5-9f20-4662-8599-fb71de28b06b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Access Inventory Snapshots"
    }
   },
   "outputs": [],
   "source": [
    "# format of inventory snapshot records\n",
    "inventory_snapshot_schema = StructType([\n",
    "  StructField('item_id', IntegerType()),\n",
    "  StructField('employee_id', IntegerType()),\n",
    "  StructField('store_id', IntegerType()),\n",
    "  StructField('date_time', TimestampType()),\n",
    "  StructField('quantity', IntegerType())\n",
    "  ])\n",
    "\n",
    "# inventory snapshot files\n",
    "inventory_snapshot_files = [ \n",
    "  config['inventory_snapshot_store001_filename'],\n",
    "  config['inventory_snapshot_online_filename']\n",
    "  ]\n",
    "\n",
    "# read inventory snapshot data\n",
    "inventory_snapshots = (\n",
    "  spark\n",
    "    .read\n",
    "    .csv(\n",
    "      inventory_snapshot_files, \n",
    "      header=True, \n",
    "      timestampFormat='yyyy-MM-dd HH:mm:ss', \n",
    "      schema=inventory_snapshot_schema\n",
    "      )\n",
    "  )\n",
    "\n",
    "display(inventory_snapshots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "192c36ec-32b2-4139-a35b-1618763f3729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To coordinate the transmission of change event data with periodic snapshots, the unique dates and times for which snapshots were taken within a given store location are extracted to a list.  This list will be used in the section of code that follows:\n",
    "\n",
    "**NOTE** It is critical for the logic below that this list of dates is sorted in chronological order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ef9c96c-14ab-47d1-aa28-0ca766950f24",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Assemble Set of Snapshot DateTimes by Store"
    }
   },
   "outputs": [],
   "source": [
    "# get date_time of each inventory snapshot by store\n",
    "inventory_snapshot_times = (\n",
    "  inventory_snapshots\n",
    "    .select('date_time','store_id')\n",
    "    .distinct()\n",
    "    .orderBy('date_time')  # sorting of list is essential for logic below\n",
    "  ).collect()\n",
    "\n",
    "# display snapshot times\n",
    "inventory_snapshot_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2360552-70bc-4f8f-aed1-f431613f8453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3: Transmit Store Data to the Cloud\n",
    "\n",
    "In this step, we will send the event change JSON documents to an Azure IOT Hub.  Using the time differences between transactions, we will delay the transmittal of each document by a calculated number of seconds. That number of seconds, derived by calculating the seconds between the current transaction and the previous transaction, is adjusted by the *event_speed_factor* variable, allowing you to speed up or slow down the replay for your needs.\n",
    "\n",
    "**NOTE** Please note that the Azure IOT Hub enforces a limit on the number of events which can be transmitted to it on a per second basis. If this limit is exceeded, you may experience *429 errors* from the IOT Hub client below. For more information this, please refer to [this document](https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-devguide-quotas-throttling).\n",
    "\n",
    "As events are transmitted, the data is checked to see if any snapshot files should be generated and sent into the Azure Storage account. In order to ensure the snapshot data is received in-sequence, any old snapshot files in the storage account are deleted before data transmission begins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47887c0e-b8ef-40d1-964c-6be227b3fef6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Delete Any Old Snapshot Files"
    }
   },
   "outputs": [],
   "source": [
    "# connect to container holding old snapshots\n",
    "blob_service_client = BlobServiceClient.from_connection_string(config['storage_connection_string'])\n",
    "container_client = blob_service_client.get_container_client(container=config['storage_container_name'])\n",
    "\n",
    "# for each blob in specified \"path\"\n",
    "for blob in container_client.list_blobs(name_starts_with=config['inventory_snapshot_path'].replace(config['dbfs_mount_name'],'')[1:]):\n",
    "  blob_client = container_client.get_blob_client(blob)\n",
    "  blob_client.delete_blob()\n",
    "\n",
    "# close clients\n",
    "container_client.close()\n",
    "blob_service_client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e207c4b-5d41-4a1f-aba8-e47aefc26620",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Connect to IOT Hub"
    }
   },
   "outputs": [],
   "source": [
    "# make sure to disconnect if this is a re-run of the notebook\n",
    "if 'client' in locals():\n",
    "  try:\n",
    "    client.disconnect()\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "# connect to iot hub\n",
    "client = IoTHubDeviceClient.create_from_connection_string( config['iot_device_connection_string'] )\n",
    "client.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aaac48ae-a000-413c-82ea-c668607202d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Send Transactions"
    }
   },
   "outputs": [],
   "source": [
    "if mode != \"prod\":\n",
    "  inventory_change = inventory_change[:100] # limit the number of transactions here to shorten execution duration; this notebook is engineered to run for approx. 3 days without this limit\n",
    "event_speed_factor = 10 # Send records to iot hub at <event_speed_factor> X real-time speed\n",
    "max_msg_size = 256 * 1024 # event message to iot hub cannot exceed 256KB\n",
    "last_dt = None\n",
    "\n",
    "for event in inventory_change:\n",
    "  # extract datetime from transaction document\n",
    "  d = eval(event) # evaluate json as a dictionary\n",
    "  dt = datetime.datetime.strptime( d['date_time'], '%Y-%m-%dT%H:%M:%S.000Z')\n",
    "  \n",
    "  # inventory snapshot transmission\n",
    "  # -----------------------------------------------------------------------\n",
    "  snapshot_start = time.time()\n",
    "  \n",
    "  inventory_snapshot_times_for_loop = inventory_snapshot_times # copy snapshot times list as this may be modified in loop\n",
    "  for snapshot_dt, store_id in inventory_snapshot_times_for_loop: # for each snapshot\n",
    "    \n",
    "    # if event date time is before next snapshot date time\n",
    "    if dt < snapshot_dt: # (snapshot times are ordered by date)\n",
    "      break              #   nothing to transmit\n",
    "      \n",
    "    else: # event date time exceeds a snapshot datetime\n",
    "      \n",
    "      # extract snapshot data for this dt\n",
    "      snapshot_pd = (\n",
    "        inventory_snapshots\n",
    "          .filter(f.expr(\"store_id={0} AND date_time='{1}'\".format(store_id, snapshot_dt)))\n",
    "          .withColumn('date_time', f.expr(\"date_format(date_time, 'yyyy-MM-dd HH:mm:ss')\")) # force timestamp conversion to include \n",
    "          .toPandas()  \n",
    "           )\n",
    "        \n",
    "      # transmit to storage blob as csv\n",
    "      blob_service_client = BlobServiceClient.from_connection_string(config['storage_connection_string'])\n",
    "      blob_client = blob_service_client.get_blob_client(\n",
    "        container=config['storage_container_name'], \n",
    "        blob=(config['inventory_snapshot_path'].replace(config['dbfs_mount_name'],'')[1:]+\n",
    "              'inventory_snapshot_{0}_{1}'.format(store_id,snapshot_dt.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "        )\n",
    "      blob_client.upload_blob(str(snapshot_pd.to_csv()), overwrite=True)\n",
    "      blob_client.close()\n",
    "      \n",
    "      # remove snapshot date from inventory_snapshot_times\n",
    "      inventory_snapshot_times.pop(0)\n",
    "      print('Loaded inventory snapshot for {0}'.format(snapshot_dt.strftime('%Y-%m-%d %H:%M:%S')))\n",
    "      \n",
    "  snapshot_seconds = time.time() - snapshot_start\n",
    "  # -----------------------------------------------------------------------\n",
    "  \n",
    "  # inventory change event transmission\n",
    "  # -----------------------------------------------------------------------\n",
    "  # calculate delay (in seconds) between this event and prior event (account for snapshot)\n",
    "  if last_dt is None: last_dt = dt\n",
    "  delay = (dt - last_dt).seconds - snapshot_seconds\n",
    "  delay = int(delay/event_speed_factor) # adjust delay by event speed factor\n",
    "  if delay < 0: delay = 0\n",
    "  \n",
    "  # sleep for delay duration\n",
    "  #print('Sleep for {0} seconds'.format(delay))\n",
    "  #time.sleep(delay)\n",
    "  \n",
    "  # send items individually if json document too large\n",
    "  # change log feb 28 2022 - added `temp = [item]` to convert dictionary to list.\n",
    "  if len(event) > max_msg_size:\n",
    "    items = d.pop('items') # retrieve items collection\n",
    "    for i, item in enumerate(items): # for each item\n",
    "      temp = [item]\n",
    "      d['items'] = temp   # add a one-item items-collection\n",
    "      client.send_message(str(d)) # send message\n",
    "\n",
    "      if (i+1)%25==0: # pause transmission every Xth item to avoid overwhelming IOT hub\n",
    "        time.sleep(1)\n",
    "  else:  # send whole transaction document\n",
    "    client.send_message(event)\n",
    "    \n",
    "  # -----------------------------------------------------------------------\n",
    "  \n",
    "  # set last_dt for next cycle\n",
    "  last_dt = dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ce80dba-c745-45ae-bd9c-d9bc72a8e3fd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Disconnect from IOT Hub"
    }
   },
   "outputs": [],
   "source": [
    "client.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1fbca624-1a0b-4bcd-afb4-0072f97c5f09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2022 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the [Databricks License](https://databricks.com/db-license-source).  All included or referenced third party libraries are subject to the licenses set forth below.\n",
    "\n",
    "| library                                | description             | license    | source                                              |\n",
    "|----------------------------------------|-------------------------|------------|-----------------------------------------------------|\n",
    "| azure-iot-device                                     | Microsoft Azure IoT Device Library | MIT    | https://pypi.org/project/azure-iot-device/                       |\n",
    "| azure-storage-blob                                | Microsoft Azure Blob Storage Client Library for Python| MIT        | https://pypi.org/project/azure-storage-blob/      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51889827-a180-4650-8640-33113d3030f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_Data Generation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
